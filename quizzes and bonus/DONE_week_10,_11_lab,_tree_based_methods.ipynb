{"cells":[{"cell_type":"markdown","id":"004a2f9a","metadata":{"id":"004a2f9a"},"source":[]},{"cell_type":"markdown","id":"934eb5b1","metadata":{"id":"934eb5b1"},"source":["# Tree-Based Methods"]},{"cell_type":"markdown","id":"54d5b3e6","metadata":{"id":"54d5b3e6"},"source":["We import some of our usual libraries at this top\n","level."]},{"cell_type":"code","execution_count":1,"id":"bXZkze28ynue","metadata":{"executionInfo":{"elapsed":12688,"status":"ok","timestamp":1743689746166,"user":{"displayName":"Joseph Williams","userId":"09916489996041413106"},"user_tz":300},"id":"bXZkze28ynue"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib.pyplot import subplots\n","import sklearn.model_selection as skm"]},{"cell_type":"code","execution_count":null,"id":"OmqNP-JLxoIJ","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"OmqNP-JLxoIJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ISLP\n","  Downloading ISLP-0.4.0-py3-none-any.whl.metadata (7.0 kB)\n","Requirement already satisfied: numpy\u003e=1.7.1 in /usr/local/lib/python3.11/dist-packages (from ISLP) (2.0.2)\n","Requirement already satisfied: scipy\u003e=0.9 in /usr/local/lib/python3.11/dist-packages (from ISLP) (1.14.1)\n","Requirement already satisfied: pandas\u003e=0.20 in /usr/local/lib/python3.11/dist-packages (from ISLP) (2.2.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from ISLP) (5.3.1)\n","Requirement already satisfied: scikit-learn\u003e=1.2 in /usr/local/lib/python3.11/dist-packages (from ISLP) (1.6.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from ISLP) (1.4.2)\n","Requirement already satisfied: statsmodels\u003e=0.13 in /usr/local/lib/python3.11/dist-packages (from ISLP) (0.14.4)\n","Collecting lifelines (from ISLP)\n","  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n","Collecting pygam (from ISLP)\n","  Downloading pygam-0.9.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from ISLP) (2.6.0+cu124)\n","Collecting pytorch-lightning (from ISLP)\n","  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n","Collecting torchmetrics (from ISLP)\n","  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas\u003e=0.20-\u003eISLP) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas\u003e=0.20-\u003eISLP) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas\u003e=0.20-\u003eISLP) (2025.2)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn\u003e=1.2-\u003eISLP) (3.6.0)\n","Requirement already satisfied: patsy\u003e=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels\u003e=0.13-\u003eISLP) (1.0.1)\n","Requirement already satisfied: packaging\u003e=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels\u003e=0.13-\u003eISLP) (24.2)\n","Requirement already satisfied: matplotlib\u003e=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines-\u003eISLP) (3.10.0)\n","Requirement already satisfied: autograd\u003e=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines-\u003eISLP) (1.7.0)\n","Collecting autograd-gamma\u003e=0.3 (from lifelines-\u003eISLP)\n","  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting formulaic\u003e=0.2.2 (from lifelines-\u003eISLP)\n","  Downloading formulaic-1.1.1-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: progressbar2\u003c5.0.0,\u003e=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pygam-\u003eISLP) (4.5.0)\n","Collecting scipy\u003e=0.9 (from ISLP)\n","  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m549.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy\u003e=1.7.1 (from ISLP)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning-\u003eISLP) (4.67.1)\n","Requirement already satisfied: PyYAML\u003e=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning-\u003eISLP) (6.0.2)\n","Requirement already satisfied: fsspec\u003e=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (2025.3.0)\n","Requirement already satisfied: typing-extensions\u003e=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning-\u003eISLP) (4.13.0)\n","Collecting lightning-utilities\u003e=0.10.0 (from pytorch-lightning-\u003eISLP)\n","  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch-\u003eISLP)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch-\u003eISLP)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch-\u003eISLP)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch-\u003eISLP)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch-\u003eISLP)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch-\u003eISLP)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch-\u003eISLP)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch-\u003eISLP)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch-\u003eISLP)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch-\u003eISLP)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch-\u003eISLP) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1-\u003etorch-\u003eISLP) (1.3.0)\n","Collecting interface-meta\u003e=1.2.0 (from formulaic\u003e=0.2.2-\u003elifelines-\u003eISLP)\n","  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: wrapt\u003e=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic\u003e=0.2.2-\u003elifelines-\u003eISLP) (1.17.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (3.11.14)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities\u003e=0.10.0-\u003epytorch-lightning-\u003eISLP) (75.2.0)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib\u003e=3.0-\u003elifelines-\u003eISLP) (1.3.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib\u003e=3.0-\u003elifelines-\u003eISLP) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib\u003e=3.0-\u003elifelines-\u003eISLP) (4.56.0)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib\u003e=3.0-\u003elifelines-\u003eISLP) (1.4.8)\n","Requirement already satisfied: pillow\u003e=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib\u003e=3.0-\u003elifelines-\u003eISLP) (11.1.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib\u003e=3.0-\u003elifelines-\u003eISLP) (3.2.3)\n","Requirement already satisfied: python-utils\u003e=3.8.1 in /usr/local/lib/python3.11/dist-packages (from progressbar2\u003c5.0.0,\u003e=4.2.0-\u003epygam-\u003eISLP) (3.9.1)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas\u003e=0.20-\u003eISLP) (1.17.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2-\u003etorch-\u003eISLP) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (1.3.2)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (25.3.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (1.5.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (6.2.0)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (0.3.1)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (1.18.3)\n","Requirement already satisfied: idna\u003e=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl\u003c2.0,\u003e=1.17.0-\u003eaiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003e=2022.5.0-\u003epytorch-lightning-\u003eISLP) (3.10)\n","Downloading ISLP-0.4.0-py3-none-any.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pygam-0.9.1-py3-none-any.whl (522 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.0/522.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.7.0-py3-none-any.whl (960 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading formulaic-1.1.1-py3-none-any.whl (115 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n","Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: autograd-gamma\n","  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=09e237b6e9a8aee21b909648e79fdb3fd3c09314e9b97b41046f1a044a0fd82e\n","  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\n","Successfully built autograd-gamma\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, lightning-utilities, interface-meta, scipy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pygam, nvidia-cusolver-cu12, formulaic, autograd-gamma, lifelines, torchmetrics, pytorch-lightning, ISLP\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.14.1\n","    Uninstalling scipy-1.14.1:\n","      Successfully uninstalled scipy-1.14.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed ISLP-0.4.0 autograd-gamma-0.5.0 formulaic-1.1.1 interface-meta-1.3.0 lifelines-0.30.0 lightning-utilities-0.14.2 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pygam-0.9.1 pytorch-lightning-2.5.1 scipy-1.11.4 torchmetrics-1.7.0\n"]}],"source":["pip install ISLP"]},{"cell_type":"code","execution_count":null,"id":"E_g6hC_KxnBq","metadata":{"colab":{"background_save":true},"id":"E_g6hC_KxnBq"},"outputs":[{"ename":"ValueError","evalue":"numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-3-3fd8882af2fa\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mISLP\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mISLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelSpec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ISLP/models/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneric_selector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 23\u001b[0;31m from .sklearn_wrap import (sklearn_sm,\n\u001b[0m\u001b[1;32m     24\u001b[0m                            \u001b[0msklearn_selected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                            sklearn_selection_path)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ISLP/models/sklearn_wrap.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneric_selector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/api.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mfamilies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m )\n\u001b[0;32m--\u003e 123\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraphics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgofplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProbPlot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqqplot_2samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbayes_mi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBayesGaussMI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/graphics/api.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtsaplots\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0magreement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_diff_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mboxplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbeanplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviolinplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcorrelation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_corr_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfactorplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minteraction_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/graphics/tsaplots.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorrelate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myule_walker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/signal/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_waveforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_max_len_seq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmax_len_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 314\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_upfirdn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupfirdn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m from ._spline import (  # noqa: F401\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/signal/_upfirdn.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_upfirdn_apply\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_output_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'upfirdn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_output_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m_upfirdn_apply.pyx\u001b[0m in \u001b[0;36minit scipy.signal._upfirdn_apply\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"]}],"source":["from ISLP import load_data, confusion_table\n","from ISLP.models import ModelSpec as MS"]},{"cell_type":"markdown","id":"8bafdee4","metadata":{"id":"8bafdee4"},"source":["We also  collect the new imports\n","needed for this lab."]},{"cell_type":"code","execution_count":null,"id":"8d6cbed4","metadata":{"colab":{"background_save":true},"id":"8d6cbed4"},"outputs":[],"source":["from sklearn.tree import (DecisionTreeClassifier as DTC,\n","                          DecisionTreeRegressor as DTR,\n","                          plot_tree,\n","                          export_text)\n","from sklearn.metrics import (accuracy_score,\n","                             log_loss)\n","from sklearn.ensemble import \\\n","     (RandomForestRegressor as RF,\n","      GradientBoostingRegressor as GBR)\n","from ISLP.bart import BART\n"]},{"cell_type":"markdown","id":"2e4ca47c","metadata":{"id":"2e4ca47c"},"source":["## Fitting Classification Trees"]},{"cell_type":"markdown","id":"b0d12b3d","metadata":{"id":"b0d12b3d"},"source":["We first use classification trees to analyze the  `Carseats`  data set.\n","In these data, `Sales` is a continuous variable, and so we begin\n","by recoding it as a binary variable. We use the `where()`\n","function to create a variable, called `High`, which takes on a\n","value of `Yes` if the `Sales` variable exceeds 8, and takes\n","on a value of `No` otherwise."]},{"cell_type":"code","execution_count":null,"id":"d85f1550","metadata":{"colab":{"background_save":true},"id":"d85f1550"},"outputs":[],"source":["Carseats = load_data('Carseats')\n","High = np.where(Carseats.Sales \u003e 8,\n","                \"Yes\",\n","                \"No\")\n"]},{"cell_type":"markdown","id":"30777bb3","metadata":{"id":"30777bb3"},"source":["We now use `DecisionTreeClassifier()`  to fit a classification tree in\n","order to predict `High` using all variables but `Sales`.\n","To do so, we must form a model matrix as we did when fitting regression\n","models.  "]},{"cell_type":"code","execution_count":null,"id":"36229722","metadata":{"colab":{"background_save":true},"id":"36229722"},"outputs":[],"source":["model = MS(Carseats.columns.drop('Sales'), intercept=False)\n","D = model.fit_transform(Carseats)\n","feature_names = list(D.columns)\n","X = np.asarray(D)\n"]},{"cell_type":"markdown","id":"a9219335","metadata":{"id":"a9219335"},"source":["We have converted `D` from a data frame to an array `X`, which is needed in some of the analysis below. We also need the `feature_names` for annotating our plots later.\n","\n","There are several options needed to specify the  classifier,\n","such as `max_depth` (how deep to grow the tree), `min_samples_split`\n","(minimum number of observations in a node to be eligible for splitting)\n","and `criterion` (whether to use Gini or cross-entropy as the split criterion).\n","We also set `random_state` for reproducibility; ties in the split criterion are broken at random."]},{"cell_type":"code","execution_count":null,"id":"587701c2","metadata":{"colab":{"background_save":true},"id":"587701c2"},"outputs":[],"source":["clf = DTC(criterion='entropy',\n","          max_depth=3,\n","          random_state=0)\n","clf.fit(X, High)\n"]},{"cell_type":"markdown","id":"c8af7dd3","metadata":{"id":"c8af7dd3"},"source":["In our discussion of qualitative features,\n","we noted that for a linear regression model such a feature could be\n","represented by including a matrix of dummy variables (one-hot-encoding) in the model\n","matrix, using the formula notation of `statsmodels`.\n","There is a more\n","natural way to handle qualitative features when building a decision\n","tree, that does not require such dummy variables; each split amounts to partitioning the levels into two groups.\n","However,\n","the `sklearn` implementation of decision trees does not take\n","advantage of this approach; instead it simply treats the one-hot-encoded levels as separate variables."]},{"cell_type":"code","execution_count":null,"id":"a0194963","metadata":{"colab":{"background_save":true},"id":"a0194963"},"outputs":[],"source":["accuracy_score(High, clf.predict(X))\n"]},{"cell_type":"markdown","id":"009ff8da","metadata":{"id":"009ff8da"},"source":["With only the default arguments, the training error rate is\n","21%.\n","For classification trees, we can\n","access the value of the deviance using `log_loss()`,\n","\\begin{equation*}\n","\\begin{split}\n","-2 \\sum_m \\sum_k n_{mk} \\log \\hat{p}_{mk},\n","\\end{split}\n","\\end{equation*}\n","where $n_{mk}$ is the number of observations in the $m$th terminal\n","node that belong to the $k$th class."]},{"cell_type":"code","execution_count":null,"id":"ef173e93","metadata":{"colab":{"background_save":true},"id":"ef173e93"},"outputs":[],"source":["resid_dev = np.sum(log_loss(High, clf.predict_proba(X)))\n","resid_dev\n"]},{"cell_type":"markdown","id":"e88410fc","metadata":{"id":"e88410fc"},"source":["This is closely related to the *entropy*, defined in (\\ref{Ch8:eq:cross-entropy}).\n","A small deviance indicates a\n","tree that provides a good fit to the (training) data.\n","\n","One of the most attractive properties of trees is that they can\n","be graphically displayed. Here we use the `plot()`  function\n","to display the tree structure."]},{"cell_type":"code","execution_count":null,"id":"06cd42f6","metadata":{"colab":{"background_save":true},"id":"06cd42f6"},"outputs":[],"source":["ax = subplots(figsize=(12,12))[1]\n","plot_tree(clf,\n","          feature_names=feature_names,\n","          ax=ax);\n"]},{"cell_type":"markdown","id":"9df3c06d","metadata":{"id":"9df3c06d"},"source":["The most important indicator of `Sales` appears to be `ShelveLoc`.\n","\n","We can see a text representation of the tree using\n","`export_text()`, which displays the split\n","criterion (e.g. `Price \u003c= 92.5`) for each branch.\n","For leaf nodes it shows the overall prediction  \n","(`Yes` or `No`).\n"," We can also see the number of observations in that\n","leaf that take on values of `Yes` and `No` by specifying  `show_weights=True`."]},{"cell_type":"code","execution_count":null,"id":"b6812ee3","metadata":{"colab":{"background_save":true},"id":"b6812ee3"},"outputs":[],"source":["print(export_text(clf,\n","                  feature_names=feature_names,\n","                  show_weights=True))\n"]},{"cell_type":"markdown","id":"182468a0","metadata":{"id":"182468a0"},"source":["In order to properly evaluate the performance of a classification tree\n","on these data, we must estimate the test error rather than simply\n","computing the training error. We split the observations into a\n","training set and a test set, build the tree using the training set,\n","and evaluate its performance on the test data. This pattern is\n","similar to that in Chapter~\\ref{Ch6:varselect}, with the linear models\n","replaced here by decision trees --- the code for validation\n","is almost identical. This approach leads to correct predictions\n","for 68.5% of the locations in the test data set."]},{"cell_type":"code","execution_count":null,"id":"7c731ea6","metadata":{"colab":{"background_save":true},"id":"7c731ea6"},"outputs":[],"source":["validation = skm.ShuffleSplit(n_splits=1,\n","                              test_size=200,\n","                              random_state=0)\n","results = skm.cross_validate(clf,\n","                             D,\n","                             High,\n","                             cv=validation)\n","results['test_score']\n"]},{"cell_type":"markdown","id":"cedd6092","metadata":{"id":"cedd6092"},"source":["    "]},{"cell_type":"markdown","id":"20619b1d","metadata":{"id":"20619b1d"},"source":["Next, we consider whether pruning the tree might lead to improved\n","classification performance. We first split the data into a training and\n","test set. We will use cross-validation to prune the tree on the training\n","set, and then evaluate the performance of the pruned tree on the test\n","set."]},{"cell_type":"code","execution_count":null,"id":"84680f6a","metadata":{"colab":{"background_save":true},"id":"84680f6a"},"outputs":[],"source":["(X_train,\n"," X_test,\n"," High_train,\n"," High_test) = skm.train_test_split(X,\n","                                   High,\n","                                   test_size=0.5,\n","                                   random_state=0)\n"]},{"cell_type":"markdown","id":"76c4fd1f","metadata":{"id":"76c4fd1f"},"source":["We first refit the full tree on the training set; here we do not set a `max_depth` parameter, since we will learn that through cross-validation.\n"]},{"cell_type":"code","execution_count":null,"id":"1c5dbf1d","metadata":{"colab":{"background_save":true},"id":"1c5dbf1d"},"outputs":[],"source":["clf = DTC(criterion='entropy', random_state=0)\n","clf.fit(X_train, High_train)\n","accuracy_score(High_test, clf.predict(X_test))\n"]},{"cell_type":"markdown","id":"4b93ca55","metadata":{"id":"4b93ca55"},"source":["Next we use the `cost_complexity_pruning_path()` method of\n","`clf` to extract cost-complexity values."]},{"cell_type":"code","execution_count":null,"id":"8cf23460","metadata":{"colab":{"background_save":true},"id":"8cf23460"},"outputs":[],"source":["ccp_path = clf.cost_complexity_pruning_path(X_train, High_train)\n","kfold = skm.KFold(10,\n","                  random_state=1,\n","                  shuffle=True)\n"]},{"cell_type":"markdown","id":"b58875f7","metadata":{"id":"b58875f7"},"source":["This yields a set of impurities and $\\alpha$ values\n","from which we can extract an optimal one by cross-validation."]},{"cell_type":"code","execution_count":null,"id":"62abb2be","metadata":{"colab":{"background_save":true},"id":"62abb2be"},"outputs":[],"source":["grid = skm.GridSearchCV(clf,\n","                        {'ccp_alpha': ccp_path.ccp_alphas},\n","                        refit=True,\n","                        cv=kfold,\n","                        scoring='accuracy')\n","grid.fit(X_train, High_train)\n","grid.best_score_\n"]},{"cell_type":"markdown","id":"0787b680","metadata":{"id":"0787b680"},"source":["Let’s take a look at the pruned true."]},{"cell_type":"code","execution_count":null,"id":"8311d318","metadata":{"colab":{"background_save":true},"id":"8311d318"},"outputs":[],"source":["ax = subplots(figsize=(12, 12))[1]\n","best_ = grid.best_estimator_\n","plot_tree(best_,\n","          feature_names=feature_names,\n","          ax=ax);\n"]},{"cell_type":"markdown","id":"09d16ee7","metadata":{"id":"09d16ee7"},"source":["This is quite a bushy tree. We could count the leaves, or query\n","`best_` instead."]},{"cell_type":"code","execution_count":null,"id":"36d1cd8f","metadata":{"colab":{"background_save":true},"id":"36d1cd8f"},"outputs":[],"source":["best_.tree_.n_leaves\n"]},{"cell_type":"markdown","id":"51e45fff","metadata":{"id":"51e45fff"},"source":["The tree with 30 terminal\n","nodes results in the lowest cross-validation error rate, with an accuracy of\n","68.5%. How well does this pruned tree perform on the test data set? Once\n","again, we apply the `predict()`  function."]},{"cell_type":"code","execution_count":null,"id":"f0f7bd8c","metadata":{"colab":{"background_save":true},"id":"f0f7bd8c"},"outputs":[],"source":["print(accuracy_score(High_test,\n","                     best_.predict(X_test)))\n","confusion = confusion_table(best_.predict(X_test),\n","                            High_test)\n","confusion\n"]},{"cell_type":"markdown","id":"a0b68fdb","metadata":{"id":"a0b68fdb"},"source":["Now 72.0% of the test observations are correctly classified, which is slightly worse than the error for the full tree (with 35 leaves). So cross-validation has not helped us much here; it only pruned off 5 leaves, at a cost of a slightly worse error. These results would change if we were to change the random number seeds above; even though cross-validation gives an unbiased approach to model selection, it does have variance.\n","\n","  "]},{"cell_type":"markdown","id":"ef41e25e","metadata":{"id":"ef41e25e"},"source":["## Fitting Regression Trees\n","Here we fit a regression tree to the  `Boston`  data set. The\n","steps are similar to those for classification trees."]},{"cell_type":"code","execution_count":null,"id":"663d8cba","metadata":{"colab":{"background_save":true},"id":"663d8cba"},"outputs":[],"source":["Boston = load_data(\"Boston\")\n","model = MS(Boston.columns.drop('medv'), intercept=False)\n","D = model.fit_transform(Boston)\n","feature_names = list(D.columns)\n","X = np.asarray(D)\n"]},{"cell_type":"markdown","id":"7af68d35","metadata":{"id":"7af68d35"},"source":["First, we split the data into training and test sets, and fit the tree\n","to the training data. Here we use 30% of the data for the test set.\n"]},{"cell_type":"code","execution_count":null,"id":"e2f6482c","metadata":{"colab":{"background_save":true},"id":"e2f6482c"},"outputs":[],"source":["(X_train,\n"," X_test,\n"," y_train,\n"," y_test) = skm.train_test_split(X,\n","                                Boston['medv'],\n","                                test_size=0.3,\n","                                random_state=0)\n"]},{"cell_type":"markdown","id":"2b25455d","metadata":{"id":"2b25455d"},"source":["Having formed  our training  and test data sets, we fit the regression tree."]},{"cell_type":"code","execution_count":null,"id":"55ff65dd","metadata":{"colab":{"background_save":true},"id":"55ff65dd"},"outputs":[],"source":["reg = DTR(max_depth=3)\n","reg.fit(X_train, y_train)\n","ax = subplots(figsize=(12,12))[1]\n","plot_tree(reg,\n","          feature_names=feature_names,\n","          ax=ax);\n"]},{"cell_type":"markdown","id":"c198059a","metadata":{"id":"c198059a"},"source":["The variable `lstat` measures the percentage of individuals with\n","lower socioeconomic status. The tree indicates that lower\n","values of `lstat` correspond to more expensive houses.\n","The tree predicts a median house price of $12,042 for small-sized homes (`rm \u003c 6.8`), in\n","suburbs in which residents have low socioeconomic status (`lstat  \u003e 14.4`) and the crime-rate is moderate (`crim \u003e 5.8`)."]},{"cell_type":"markdown","id":"6448ca9c","metadata":{"id":"6448ca9c"},"source":["Now we use the cross-validation function to see whether pruning\n","the tree will improve performance."]},{"cell_type":"code","execution_count":null,"id":"681b183e","metadata":{"colab":{"background_save":true},"id":"681b183e"},"outputs":[],"source":["ccp_path = reg.cost_complexity_pruning_path(X_train, y_train)\n","kfold = skm.KFold(5,\n","                  shuffle=True,\n","                  random_state=10)\n","grid = skm.GridSearchCV(reg,\n","                        {'ccp_alpha': ccp_path.ccp_alphas},\n","                        refit=True,\n","                        cv=kfold,\n","                        scoring='neg_mean_squared_error')\n","G = grid.fit(X_train, y_train)\n"]},{"cell_type":"markdown","id":"1e802274","metadata":{"id":"1e802274"},"source":["In keeping with the cross-validation results, we use the pruned tree\n","to make predictions on the test set."]},{"cell_type":"code","execution_count":null,"id":"78a255e3","metadata":{"colab":{"background_save":true},"id":"78a255e3"},"outputs":[],"source":["best_ = grid.best_estimator_\n","np.mean((y_test - best_.predict(X_test))**2)\n"]},{"cell_type":"markdown","id":"1be24e79","metadata":{"id":"1be24e79"},"source":["In other words, the test set MSE associated with the regression tree\n","is 28.07.  The square root of\n","the MSE is therefore around\n","5.30,\n","indicating that this model leads to test predictions that are within around\n","$5300\n","of the true median home value for the suburb.\n","\n","Let’s plot the best tree to see how interpretable it is."]},{"cell_type":"code","execution_count":null,"id":"d6f01ff8","metadata":{"colab":{"background_save":true},"id":"d6f01ff8"},"outputs":[],"source":["ax = subplots(figsize=(12,12))[1]\n","plot_tree(G.best_estimator_,\n","          feature_names=feature_names,\n","          ax=ax);\n"]},{"cell_type":"markdown","id":"cd491f07","metadata":{"id":"cd491f07"},"source":["\n"]},{"cell_type":"markdown","id":"6fcf4ce3","metadata":{"id":"6fcf4ce3"},"source":["## Bagging and Random Forests"]},{"cell_type":"markdown","id":"6bab3fe7","metadata":{"id":"6bab3fe7"},"source":["Here we apply bagging and random forests to the `Boston` data, using\n","the `RandomForestRegressor()` from the `sklearn.ensemble` package. Recall\n","that bagging is simply a special case of a random forest with\n","$m=p$. Therefore, the `RandomForestRegressor()`  function can be used to\n","perform both bagging and random forests. We start with bagging."]},{"cell_type":"code","execution_count":null,"id":"6f7ba658","metadata":{"colab":{"background_save":true},"id":"6f7ba658"},"outputs":[],"source":["bag_boston = RF(max_features=X_train.shape[1], random_state=0)\n","bag_boston.fit(X_train, y_train)\n"]},{"cell_type":"markdown","id":"ff30e5c3","metadata":{"id":"ff30e5c3"},"source":["The argument `max_features` indicates that all 12 predictors should\n","be considered for each split of the tree --- in other words, that\n","bagging should be done.  How well does this bagged model perform on\n","the test set?"]},{"cell_type":"code","execution_count":null,"id":"82e484c3","metadata":{"colab":{"background_save":true},"id":"82e484c3"},"outputs":[],"source":["ax = subplots(figsize=(8,8))[1]\n","y_hat_bag = bag_boston.predict(X_test)\n","ax.scatter(y_hat_bag, y_test)\n","np.mean((y_test - y_hat_bag)**2)\n"]},{"cell_type":"markdown","id":"9312ebcc","metadata":{"id":"9312ebcc"},"source":["The test set MSE associated with the bagged regression tree is\n","14.63, about half that obtained using an optimally-pruned single\n","tree.  We could change the number of trees grown from the default of\n","100 by\n","using the `n_estimators` argument:"]},{"cell_type":"code","execution_count":null,"id":"c8de558e","metadata":{"colab":{"background_save":true},"id":"c8de558e"},"outputs":[],"source":["bag_boston = RF(max_features=X_train.shape[1],\n","                n_estimators=500,\n","                random_state=0).fit(X_train, y_train)\n","y_hat_bag = bag_boston.predict(X_test)\n","np.mean((y_test - y_hat_bag)**2)"]},{"cell_type":"markdown","id":"7841f5f8","metadata":{"id":"7841f5f8"},"source":["There is not much change. Bagging and random forests cannot overfit by\n","increasing the number of trees, but can underfit if the number is too small.\n","\n","Growing a random forest proceeds in exactly the same way, except that\n","we use a smaller value of the `max_features` argument. By default,\n","`RandomForestRegressor()`  uses $p$ variables when building a random\n","forest of regression trees (i.e. it defaults to bagging), and `RandomForestClassifier()` uses\n","$\\sqrt{p}$ variables when building a\n","random forest of classification trees. Here we use `max_features=6`."]},{"cell_type":"code","execution_count":null,"id":"bbebeb55","metadata":{"colab":{"background_save":true},"id":"bbebeb55"},"outputs":[],"source":["RF_boston = RF(max_features=6,\n","               random_state=0).fit(X_train, y_train)\n","y_hat_RF = RF_boston.predict(X_test)\n","np.mean((y_test - y_hat_RF)**2)\n"]},{"cell_type":"markdown","id":"5e2a4fa4","metadata":{"id":"5e2a4fa4"},"source":["The test set MSE is 20.04;\n","this indicates that random forests did somewhat worse than bagging\n","in this case. Extracting the `feature_importances_` values from the fitted model, we can view the\n","importance of each variable."]},{"cell_type":"code","execution_count":null,"id":"36dcca85","metadata":{"colab":{"background_save":true},"id":"36dcca85"},"outputs":[],"source":["feature_imp = pd.DataFrame(\n","    {'importance':RF_boston.feature_importances_},\n","    index=feature_names)\n","feature_imp.sort_values(by='importance', ascending=False)"]},{"cell_type":"markdown","id":"a7ed872f","metadata":{"id":"a7ed872f"},"source":[" This\n","is a relative measure of the total decrease in node impurity that results from\n","splits over that variable, averaged over all trees (this was plotted in Figure~\\ref{Ch8:fig:varimp} for a model fit to the `Heart` data).\n","\n","The results indicate that across all of the trees considered in the\n","random forest, the wealth level of the community (`lstat`) and the\n","house size (`rm`) are by far the two most important variables.\n","\n","  "]},{"cell_type":"markdown","id":"cd8a07e7","metadata":{"id":"cd8a07e7"},"source":["## Boosting"]},{"cell_type":"markdown","id":"d491bcd5","metadata":{"id":"d491bcd5"},"source":["Here we use `GradientBoostingRegressor()` from `sklearn.ensemble`\n","to fit boosted regression trees to the `Boston` data\n","set. For classification we would  use `GradientBoostingClassifier()`.\n","The argument `n_estimators=5000`\n","indicates that we want 5000 trees, and the option\n","`max_depth=3` limits the depth of each tree. The\n","argument `learning_rate` is the $\\lambda$\n","mentioned earlier in the description of boosting."]},{"cell_type":"code","execution_count":null,"id":"4478fc0c","metadata":{"colab":{"background_save":true},"id":"4478fc0c"},"outputs":[],"source":["boost_boston = GBR(n_estimators=5000,\n","                   learning_rate=0.001,\n","                   max_depth=3,\n","                   random_state=0)\n","boost_boston.fit(X_train, y_train)\n"]},{"cell_type":"markdown","id":"4046f1a2","metadata":{"id":"4046f1a2"},"source":["We can see how the training error decreases with the `train_score_` attribute.\n","To get an idea of how the test error decreases we can use the\n","`staged_predict()` method to get the predicted values along the path."]},{"cell_type":"code","execution_count":null,"id":"57ac9a1e","metadata":{"colab":{"background_save":true},"id":"57ac9a1e"},"outputs":[],"source":["test_error = np.zeros_like(boost_boston.train_score_)\n","for idx, y_ in enumerate(boost_boston.staged_predict(X_test)):\n","   test_error[idx] = np.mean((y_test - y_)**2)\n","\n","plot_idx = np.arange(boost_boston.train_score_.shape[0])\n","ax = subplots(figsize=(8,8))[1]\n","ax.plot(plot_idx,\n","        boost_boston.train_score_,\n","        'b',\n","        label='Training')\n","ax.plot(plot_idx,\n","        test_error,\n","        'r',\n","        label='Test')\n","ax.legend();\n"]},{"cell_type":"markdown","id":"dd6568e3","metadata":{"id":"dd6568e3"},"source":["We now use the boosted model to predict `medv` on the test set:"]},{"cell_type":"code","execution_count":null,"id":"5e9127e2","metadata":{"colab":{"background_save":true},"id":"5e9127e2"},"outputs":[],"source":["y_hat_boost = boost_boston.predict(X_test);\n","np.mean((y_test - y_hat_boost)**2)\n"]},{"cell_type":"markdown","id":"77eb1bc9","metadata":{"id":"77eb1bc9"},"source":[" The test MSE obtained is 14.48,\n","similar to the test MSE for bagging. If we want to, we can\n","perform boosting with a different value of the shrinkage parameter\n","$\\lambda$ in  (\\ref{Ch8:alphaboost}). The default value is 0.001, but\n","this is easily modified.  Here we take $\\lambda=0.2$."]},{"cell_type":"code","execution_count":null,"id":"f76d94d9","metadata":{"colab":{"background_save":true},"id":"f76d94d9"},"outputs":[],"source":["boost_boston = GBR(n_estimators=5000,\n","                   learning_rate=0.2,\n","                   max_depth=3,\n","                   random_state=0)\n","boost_boston.fit(X_train,\n","                 y_train)\n","y_hat_boost = boost_boston.predict(X_test);\n","np.mean((y_test - y_hat_boost)**2)\n"]},{"cell_type":"markdown","id":"5f4ff1aa","metadata":{"id":"5f4ff1aa"},"source":["In this case, using $\\lambda=0.2$ leads to a almost the same test MSE\n","as when using $\\lambda=0.001$.\n","\n"]},{"cell_type":"markdown","id":"d1311869","metadata":{"id":"d1311869"},"source":["## Bayesian Additive Regression Trees"]},{"cell_type":"markdown","id":"b703f799","metadata":{"id":"b703f799"},"source":["In this section we demonstrate a  `Python` implementation of BART found in the\n","`ISLP.bart` package. We fit a  model\n","to the `Boston` housing data set. This `BART()` estimator is\n","designed for quantitative outcome variables, though other implementations are available for\n","fitting logistic and probit models to categorical outcomes."]},{"cell_type":"code","execution_count":null,"id":"a310d4bd","metadata":{"colab":{"background_save":true},"id":"a310d4bd"},"outputs":[],"source":["bart_boston = BART(random_state=0, burnin=5, ndraw=15)\n","bart_boston.fit(X_train, y_train)\n"]},{"cell_type":"markdown","id":"dfbffc72","metadata":{"id":"dfbffc72"},"source":["On this data set, with this split into test and training, we see that the test error of BART is similar to that of  random forest."]},{"cell_type":"code","execution_count":null,"id":"4e45c7e0","metadata":{"colab":{"background_save":true},"id":"4e45c7e0"},"outputs":[],"source":["yhat_test = bart_boston.predict(X_test.astype(np.float32))\n","np.mean((y_test - yhat_test)**2)\n"]},{"cell_type":"markdown","id":"0de5badb","metadata":{"id":"0de5badb"},"source":["We can check how many times each variable appeared in the collection of trees.\n","This gives a summary similar to the variable importance plot for boosting and random forests."]},{"cell_type":"code","execution_count":null,"id":"77037b29","metadata":{"colab":{"background_save":true},"id":"77037b29"},"outputs":[],"source":["var_inclusion = pd.Series(bart_boston.variable_inclusion_.mean(0),\n","                               index=D.columns)\n","var_inclusion\n"]},{"cell_type":"markdown","id":"d60caee3","metadata":{"id":"d60caee3"},"source":["    \n","  \n","\n","\n"]}],"metadata":{"colab":{"name":"","version":""},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}